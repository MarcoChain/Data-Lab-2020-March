{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\marco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import icu\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "import re\n",
    "#Classifier\n",
    "import spacy\n",
    "from sklearn import linear_model\n",
    "from sklearn.kernel_approximation import AdditiveChi2Sampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LassoCV\n",
    "#Preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_selection import SelectFpr, chi2\n",
    "from scipy.stats import rankdata\n",
    "from scipy.stats.mstats import describe\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.stats.mstats import gmean\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2,mutual_info_classif,f_regression\n",
    "from sklearn.feature_selection import SelectPercentile, chi2,SelectFpr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import random\n",
    "import nltk\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords as sw\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MaxAbsScaler \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords as sw\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from num2words import num2words \n",
    "import multidict\n",
    "import string\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "#Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering,MiniBatchKMeans,SpectralClustering, MeanShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_csv('development.csv')\n",
    "ev = pd.read_csv('evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ita_stemmer = nltk.stem.snowball.ItalianStemmer()\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self): \n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    def __call__(self, documents): \n",
    "        lemmas = [] \n",
    "        #print(nltk.pos_tag( word_tokenize(documents) ))\n",
    "        for t in word_tokenize(documents):\n",
    "            t = t.strip()\n",
    "            lemma = self.lemmatizer.lemmatize(t) \n",
    "            lemmas.append(ita_stemmer.stem(lemma)) \n",
    "            #lemmas.append(lemma) \n",
    "        return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=list(string.punctuation) + nltk.corpus.stopwords.words('italian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712\n"
     ]
    }
   ],
   "source": [
    "#d1=[\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\",'from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come','seen']\n",
    "d4 = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "voc = [\"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"q\",\"w\",\"e\",\"r\",\"t\",\"y\",\"u\",\"i\",\"o\",\"p\",\"a\",\"s\",\"d\",\"f\",\"g\",\"h\",\"j\",\"k\",\"l\",\"z\",\"x\",\"c\",\"v\",\"b\",\"n\",\"m\"]\n",
    "#d5 = [\"a\",\"â\",\"à\",\"z\",\"y\",\"x\",\"w\",\"v\",\"u\",\"t\",\"s\",\"r\",\"q\",\"p\",\"o\",\"n\",\"m\",\"l\",\"k\",\"j\",\"i\",\"h\",\"g\",\"f\",\"e\",\"d\",\"c\",\"b\",\"ô\",\"qu\",\"pu\",\"sa\",\"se\",\"où\",\"o|\",\"ou\",\"on\",\"oh\",\"ni\",\"si\",\"ne\",\"na\",\"me\",\"ma\",\"là\",\"le\",\"ta\",\"la\",\"je\",\"te\",\"il\",\"hé\",\"ho\",\"hi\",\"ha\",\"as\",\"fi\",\"eu\",\"et\",\"es\",\"en\",\"eh\",\"du\",\"tu\",\"té\",\"un\",\"va\",\"de\",\"da\",\"au\",\"ci\",\"vu\",\"vé\",\"ai\",\"ah\",\"ça\",\"ès\",\"ce\",\"ohé\",\"hem\",\"car\",\"hop\",\"hou\",\"fût\",\"pur\",\"fut\",\"hue\",\"hui\",\"tel\",\"bon\",\"que\",\"fus\",\"zut\",\"hum\",\"ici\",\"aux\",\"ils\",\"tac\",\"aie\",\"qui\",\"été\",\"ces\",\"cet\",\"ait\",\"sur\",\"eût\",\"las\",\"les\",\"tes\",\"tic\",\"eux\",\"eut\",\"toc\",\"lui\",\"eus\",\"euh\",\"pif\",\"bat\",\"eue\",\"etc\",\"lès\",\"vos\",\"est\",\"toi\",\"mes\",\"ton\",\"pff\",\"moi\",\"son\",\"mon\",\"mot\",\"soi\",\"peu\",\"dès\",\"six\",\"non\",\"bas\",\"nos\",\"bah\",\"pas\",\"dos\",\"ses\",\"dix\",\"nul\",\"hep\",\"dit\",\"par\",\"pan\",\"olé\",\"des\",\"vif\",\"via\",\"ont\",\"vas\",\"paf\",\"une\",\"uns\",\"ore\",\"ouf\",\"oust\",\"aura\",\"étés\",\"vais\",\"allo\",\"onze\",\"unes\",\"deux\",\"vers\",\"aies\",\"dire\",\"ayez\",\"vifs\",\"vive\",\"dite\",\"dits\",\"ollé\",\"sera\",\"delà\",\"deja\",\"très\",\"trop\",\"nous\",\"tres\",\"doit\",\"sept\",\"donc\",\"dont\",\"sent\",\"dans\",\"sein\",\"seul\",\"neuf\",\"sien\",\"crac\",\"allô\",\"sauf\",\"même\",\"peut\",\"sois\",\"vlan\",\"sans\",\"tout\",\"soit\",\"voie\",\"elle\",\"tous\",\"peux\",\"pfft\",\"sont\",\"mine\",\"sous\",\"mien\",\"clic\",\"clac\",\"meme\",\"mais\",\"vont\",\"avez\",\"pfut\",\"vous\",\"etre\",\"avec\",\"cinq\",\"eues\",\"pire\",\"stop\",\"chut\",\"sait\",\"chez\",\"suis\",\"lors\",\"suit\",\"plus\",\"cher\",\"tien\",\"rien\",\"beau\",\"leur\",\"pour\",\"rend\",\"rare\",\"ceux\",\"étée\",\"fais\",\"tend\",\"quoi\",\"fait\",\"bien\",\"tant\",\"pres\",\"afin\",\"flac\",\"floc\",\"fois\",\"font\",\"cent\",\"celà\",\"quel\",\"boum\",\"tels\",\"huit\",\"près\",\"êtes\",\"puis\",\"houp\",\"cela\",\"ceci\",\"hors\",\"gens\",\"pure\",\"holà\",\"haut\",\"hein\",\"être\",\"brrr\",\"état\",\"étiez\",\"avais\",\"après\",\"quand\",\"avait\",\"fûtes\",\"fûmes\",\"telle\",\"aucun\",\"bravo\",\"fusse\",\"celle\",\"était\",\"étais\",\"hélas\",\"apres\",\"force\",\"avant\",\"psitt\",\"celui\",\"façon\",\"abord\",\"jusqu\",\"bigre\",\"juste\",\"quels\",\"cette\",\"vôtre\",\"assez\",\"eûtes\",\"eûmes\",\"autre\",\"rares\",\"tenir\",\"sujet\",\"étant\",\"tente\",\"leurs\",\"pouah\",\"tiens\",\"reste\",\"eusse\",\"ainsi\",\"chers\",\"aussi\",\"etant\",\"alors\",\"plouf\",\"maint\",\"plein\",\"essai\",\"pièce\",\"chère\",\"soyez\",\"votre\",\"aviez\",\"memes\",\"merci\",\"voilà\",\"entre\",\"comme\",\"enfin\",\"miens\",\"mille\",\"mince\",\"voici\",\"vives\",\"elles\",\"avoir\",\"couic\",\"pense\",\"moins\",\"egale\",\"effet\",\"toute\",\"basee\",\"passé\",\"début\",\"mêmes\",\"seize\",\"étées\",\"selon\",\"parmi\",\"sinon\",\"parle\",\"siens\",\"serai\",\"avons\",\"parce\",\"aurez\",\"auras\",\"seule\",\"dring\",\"vivat\",\"vingt\",\"douze\",\"aurai\",\"notre\",\"trois\",\"outre\",\"ouste\",\"ouias\",\"tsoin\",\"serez\",\"nôtre\",\"devra\",\"aient\",\"seras\",\"ayons\",\"ayant\",\"quant\",\"aurais\",\"tsouin\",\"unique\",\"aurait\",\"allons\",\"aucune\",\"nôtres\",\"seriez\",\"devers\",\"serons\",\"seront\",\"ouvert\",\"serait\",\"serais\",\"trente\",\"devant\",\"valeur\",\"dessus\",\"treize\",\"nommés\",\"droite\",\"sienne\",\"auriez\",\"depuis\",\"parler\",\"aucuns\",\"parole\",\"dehors\",\"dedans\",\"semble\",\"duquel\",\"durant\",\"debout\",\"toutes\",\"soient\",\"egales\",\"permet\",\"auquel\",\"sommes\",\"contre\",\"avions\",\"encore\",\"voient\",\"mienne\",\"envers\",\"aurons\",\"chères\",\"malgré\",\"malgre\",\"soyons\",\"auront\",\"chiche\",\"eurent\",\"aupres\",\"plutôt\",\"suffit\",\"chaque\",\"retour\",\"eusses\",\"tienne\",\"chacun\",\"rendre\",\"suivre\",\"lequel\",\"tenant\",\"faites\",\"certes\",\"quinze\",\"vôtres\",\"tandis\",\"jusque\",\"quelle\",\"autres\",\"autrui\",\"proche\",\"celles\",\"feront\",\"étions\",\"hurrah\",\"quatre\",\"furent\",\"fusses\",\"quanta\",\"telles\",\"hormis\",\"divers\",\"devrait\",\"fussent\",\"importe\",\"tardive\",\"puisque\",\"aujourd\",\"uniques\",\"comment\",\"combien\",\"faisant\",\"aurions\",\"laisser\",\"quelles\",\"dessous\",\"quelque\",\"surtout\",\"extenso\",\"excepté\",\"compris\",\"premier\",\"directe\",\"quoique\",\"suivant\",\"pouvait\",\"tiennes\",\"lorsque\",\"eussiez\",\"eussent\",\"avaient\",\"chacune\",\"ceux-là\",\"restant\",\"diverse\",\"restent\",\"ceux-ci\",\"environ\",\"revoici\",\"revoilà\",\"souvent\",\"miennes\",\"sixième\",\"naturel\",\"plupart\",\"peuvent\",\"dernier\",\"siennes\",\"certain\",\"pendant\",\"attendu\",\"étaient\",\"nouveau\",\"dixième\",\"doivent\",\"serions\",\"partant\",\"parseme\",\"onzième\",\"parlent\",\"parfois\",\"ouverte\",\"ouverts\",\"fussiez\",\"septième\",\"douzième\",\"nouveaux\",\"semblent\",\"dix-sept\",\"celui-ci\",\"celle-là\",\"nombreux\",\"celui-là\",\"toujours\",\"neuvième\",\"touchant\",\"personne\",\"derniere\",\"sapristi\",\"certaine\",\"dix-neuf\",\"dix-huit\",\"toi-même\",\"soi-même\",\"multiple\",\"lesquels\",\"moindres\",\"soixante\",\"moi-même\",\"moi-meme\",\"minimale\",\"diverses\",\"celle-ci\",\"maximale\",\"possible\",\"certains\",\"beaucoup\",\"pourquoi\",\"lui-même\",\"pourrais\",\"subtiles\",\"auxquels\",\"relative\",\"lui-meme\",\"pourrait\",\"rarement\",\"derriere\",\"suivante\",\"eussions\",\"suivants\",\"derrière\",\"première\",\"desquels\",\"laquelle\",\"quelques\",\"ailleurs\",\"probable\",\"probante\",\"deuxième\",\"auraient\",\"quatorze\",\"quarante\",\"huitième\",\"fussions\",\"allaient\",\"seraient\",\"tellement\",\"anterieur\",\"faisaient\",\"exterieur\",\"superpose\",\"cinquième\",\"naturelle\",\"eux-mêmes\",\"suivantes\",\"longtemps\",\"cinquante\",\"suffisant\",\"certaines\",\"multiples\",\"neanmoins\",\"cependant\",\"seulement\",\"elle-même\",\"egalement\",\"notamment\",\"toutefois\",\"désormais\",\"néanmoins\",\"celles-là\",\"celles-ci\",\"semblable\",\"troisième\",\"personnes\",\"sacrebleu\",\"plusieurs\",\"possessif\",\"possibles\",\"prealable\",\"autrement\",\"quiconque\",\"desormais\",\"autrefois\",\"différent\",\"quelqu'un\",\"procedant\",\"quatrième\",\"moyennant\",\"different\",\"auxquelles\",\"semblaient\",\"differents\",\"quelconque\",\"différente\",\"desquelles\",\"nous-mêmes\",\"différents\",\"absolument\",\"nombreuses\",\"restrictif\",\"concernant\",\"exactement\",\"necessaire\",\"comparable\",\"naturelles\",\"anterieure\",\"lesquelles\",\"possessifs\",\"suffisante\",\"maintenant\",\"vous-mêmes\",\"specifique\",\"speculatif\",\"specifiques\",\"strictement\",\"remarquable\",\"anterieures\",\"comparables\",\"elles-mêmes\",\"precisement\",\"directement\",\"différentes\",\"particulier\",\"aujourd'hui\",\"differentes\",\"quant-à-soi\",\"particulière\",\"quatre-vingt\",\"cinquantaine\",\"relativement\",\"cinquantième\",\"uniformement\",\"premièrement\",\"deuxièmement\",\"quatrièmement\",\"troisièmement\",\"necessairement\",\"particulièrement\"]\n",
    "#d3 =  list(string.punctuation) + ['``',\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would','le', 'u', \"'m\", '0', 'affect', 'ai', 'change', 'greeting', 'pa', 'regard', 'result', 'word','c', 'ca', 'v', 'bit', 'two','easy', 'enough', 'btw','small','new','u','mail','improve','item','via','recently','mailing','subject','like','still','g','undestand', 'yet','somewhere','two','got','making','r','2.','2','soon','think','perhaps','open','small','well','try','sorry','lot','get','thanks','hold','note','try','discovered','heard','read','going','received','say','useful','person','thus','seems','way','several','good','seen','set','take','new','used','started', 'know',\"'just\",'read','gradually','somewhere','select','enough','house','clarification','manage','case','understand','someone','read']\n",
    "#d4 = [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'doe', 'ha', 'le', \"n't\", 'sha', 'u', 'wa', 'wo']\n",
    "#stop = list(set(sw.words('english') + d1 + d2+d3+d4))\n",
    "d6 = ['adess', 'allor', 'ancor', 'are', 'aver', 'aves', 'buon', 'cinqu', 'compr', 'consecut', 'cos', 'dentr', 'dev', 'dopp', 'fo', 'gent', 'indietr', 'invec', 'lavor', 'megl', 'molt', 'nom', 'not', 'nov', 'nuov', 'pen', 'person', 'ple', 'poc', 'prim', 'promess', 'quart', 'quas', 'quattr', 'quind', 'quint', 'second', 'senz', 'sett', 'sol', 'sopr', 'soprattutt', 'sott', 'stat', 'sub', 'tant', 'te', 'temp', 'terz', 'tho', 'tripl', 'ultim', 'volt', 'who']\n",
    "d5 = ['abbi', 'abbiam', 'abov', 'aggiunt', 'alon', 'althoug', 'andar', 'anyon', 'anywher', 'appars', 'appen', 'appes', 'arriv', 'avemm', 'avend', 'avess', 'avesser', 'avessim', 'avest', 'avet', 'avev', 'avevam', 'avra', 'avrann', 'avre', 'avrebb', 'avrebber', 'avrem', 'avremm', 'avrest', 'avret', 'avut', 'bagagl', 'becam', 'becaus', 'becom', 'befor', 'besid', 'bimb', \"c'er\", 'capit', 'com', 'compagn', 'consider', 'contr', 'davver', 'decis', 'describ', 'don', 'ebber', 'elsewh', 'enoug', 'eran', 'erav', 'eravam', 'essend', 'everyon', 'everywh', 'fac', 'facc', 'facess', 'facessim', 'facest', 'fann', 'far', 'fara', 'farann', 'fare', 'farebb', 'farebber', 'farem', 'farest', 'fec', 'fecer', 'fidanz', 'fin', 'fir', 'fiv', 'form', 'foss', 'fosser', 'fossim', 'fost', 'fumm', 'fur', 'giv', 'hann', 'hav', 'henc', 'iniz', \"l'abb\", 'lor', 'lung', 'mad', 'meanwhil', 'min', 'mor', 'mov', 'nam', 'neanc', 'nin', 'nonc', 'noon', 'nostr', 'nowh', 'otherwis', 'otten', 'parol', 'pens', 'perc', 'pleas', 'poch', 'pres', 'prezz', 'prov', 'punt', 'qual', 'qualsias', 'quant', 'quantum', 'quell', 'quest', 'ragazz', 'recension', 'sam', 'sar', 'sara', 'sarann', 'sare', 'sarebb', 'sarebber', 'sarem', 'sarest', 'scors', 'sembr', 'sent', 'siam', 'sian', 'siat', 'sid', 'siet', 'sinc', 'sinistr', 'som', 'someon', 'sometim', 'somewh', 'son', 'stand', 'stann', 'star', 'stara', 'starann', 'starebb', 'starebber', 'starem', 'starest', 'stav', 'stavam', 'stemm', 'stess', 'stesser', 'stessim', 'stest', 'stett', 'stetter', 'sti', 'stiam', 'stil', 'tak', 'thenc', 'ther', 'therefor', 'thes', 'thos', 'thoug', 'thre', 'throug', 'tratt', 'trov', 'tutt', 'twelv', 'u', 'val', 'verit', 'vol', 'vostr', 'wa', 'wer', 'whenc', 'wher', 'whic', 'whil', 'whol', 'whos']\n",
    "d8 = list(string.punctuation) +[\"a\",\"abbastanza\",\"abbia\",\"abbiamo\",\"abbiano\",\"abbiate\",\"accidenti\",\"ad\",\"adesso\",\"affinche\",\"agl\",\"agli\",\"ahime\",\"ahimã¨\",\"ahimè\",\"ai\",\"al\",\"alcuna\",\"alcuni\",\"alcuno\",\"all\",\"alla\",\"alle\",\"allo\",\"allora\",\"altre\",\"altri\",\"altrimenti\",\"altro\",\"altrove\",\"altrui\",\"anche\",\"ancora\",\"anni\",\"anno\",\"ansa\",\"anticipo\",\"assai\",\"attraverso\",\"avanti\",\"avemmo\",\"avendo\",\"avente\",\"aver\",\"avere\",\"averlo\",\"avesse\",\"avessero\",\"avessi\",\"avessimo\",\"aveste\",\"avesti\",\"avete\",\"aveva\",\"avevamo\",\"avevano\",\"avevate\",\"avevi\",\"avevo\",\"avrai\",\"avranno\",\"avrebbe\",\"avrebbero\",\"avrei\",\"avremmo\",\"avremo\",\"avreste\",\"avresti\",\"avrete\",\"avrà\",\"avrò\",\"avuta\",\"avute\",\"avuti\",\"avuto\",\"berlusconi\",\"c\",\"caso\",\"cento\",\"certa\",\"certe\",\"certi\",\"che\",\"chi\",\"chicchessia\",\"chiunque\",\"ci\",\"ciascuna\",\"ciascuno\",\"cima\",\"cinque\",\"cio\",\"cioe\",\"cioã¨\",\"cioè\",\"circa\",\"ciã²\",\"ciò\",\"co\",\"codesta\",\"codesti\",\"codesto\",\"cogli\",\"coi\",\"col\",\"colei\",\"coll\",\"coloro\",\"colui\",\"come\",\"comprare\",\"comunque\",\"con\",\"concernente\",\"conciliarsi\",\"conclusione\",\"consecutivi\",\"consecutivo\",\"cos\",\"cosa\",\"cosi\",\"cosã¬\",\"così\",\"cui\",\"d\",\"da\",\"dagl\",\"dagli\",\"dai\",\"dal\",\"dall\",\"dalla\",\"dalle\",\"dallo\",\"dappertutto\",\"davanti\",\"degl\",\"degli\",\"dei\",\"del\",\"dell\",\"della\",\"delle\",\"dello\",\"dentro\",\"detto\",\"deve\",\"devo\",\"di\",\"dice\",\"dietro\",\"dire\",\"dirimpetto\",\"diventa\",\"diventare\",\"diventato\",\"dopo\",\"doppio\",\"dov\",\"dove\",\"dovra\",\"dovrà\",\"dovrã\",\"dovunque\",\"due\",\"dunque\",\"durante\",\"e\",\"ebbe\",\"ebbero\",\"ebbi\",\"ecc\",\"ecco\",\"ed\",\"effettivamente\",\"egli\",\"ella\",\"entrambi\",\"eppure\",\"era\",\"erano\",\"eravamo\",\"eravate\",\"eri\",\"ero\",\"esempio\",\"esse\",\"essendo\",\"esser\",\"essere\",\"essi\",\"ex\",\"fa\",\"faccia\",\"facciamo\",\"facciano\",\"facciate\",\"faccio\",\"facemmo\",\"facendo\",\"facesse\",\"facessero\",\"facessi\",\"facessimo\",\"faceste\",\"facesti\",\"faceva\",\"facevamo\",\"facevano\",\"facevate\",\"facevi\",\"facevo\",\"fai\",\"fanno\",\"farai\",\"faranno\",\"fare\",\"farebbe\",\"farebbero\",\"farei\",\"faremmo\",\"faremo\",\"fareste\",\"faresti\",\"farete\",\"farà\",\"farò\",\"fatto\",\"favore\",\"fece\",\"fecero\",\"feci\",\"fin\",\"finche\",\"fine\",\"fino\",\"forse\",\"fosse\",\"fossero\",\"fossi\",\"fossimo\",\"foste\",\"fosti\",\"fra\",\"frattempo\",\"fu\",\"fui\",\"fummo\",\"furono\",\"generale\",\"gia\",\"giacche\",\"giorni\",\"giorno\",\"giu\",\"già\",\"giã\",\"gli\",\"gliela\",\"gliele\",\"glieli\",\"glielo\",\"gliene\",\"governo\",\"grande\",\"gruppo\",\"ha\",\"haha\",\"hai\",\"hanno\",\"ho\",\"i\",\"ie\",\"ieri\",\"il\",\"improvviso\",\"in\",\"inc\",\"indietro\",\"infatti\",\"inoltre\",\"insieme\",\"intanto\",\"intorno\",\"invece\",\"io\",\"l\",\"la\",\"lasciato\",\"lato\",\"lavoro\",\"le\",\"lei\",\"li\",\"lo\",\"loro\",\"lui\",\"là\",\"lã\",\"ma\",\"macche\",\"magari\",\"me\",\"medesimo\",\"mediante\",\"mentre\",\"mesi\",\"mezzo\",\"mi\",\"mia\",\"mie\",\"miei\",\"ministro\",\"mio\",\"modo\",\"momento\",\"mondo\",\"mosto\",\"nazionale\",\"ne\",\"negl\",\"negli\",\"nei\",\"nel\",\"nell\",\"nella\",\"nelle\",\"nello\",\"nemmeno\",\"neppure\",\"nessun\",\"nessuna\",\"nessuno\",\"niente\",\"no\",\"noi\",\"nome\",\"non\",\"nondimeno\",\"nonostante\",\"nonsia\",\"nostra\",\"nostre\",\"nostri\",\"nostro\",\"novanta\",\"nove\",\"nuovi\",\"o\",\"od\",\"oggi\",\"ogni\",\"ognuna\",\"ognuno\",\"oltre\",\"oppure\",\"ora\",\"ore\",\"osi\",\"ossia\",\"ottanta\",\"per\",\"perche\",\"perchã¨\",\"perchè\",\"perché\",\"percio\",\"perciã²\",\"perciò\",\"perfino\",\"pero\",\"persino\",\"persone\",\"perã²\",\"però\",\"piglia\",\"piu\",\"piuttosto\",\"piã¹\",\"più\",\"po\",\"poi\",\"poiche\",\"possa\",\"possedere\",\"posteriore\",\"press\",\"prima\",\"primo\",\"principalmente\",\"probabilmente\",\"promesso\",\"proprio\",\"puo\",\"pure\",\"purtroppo\",\"puã²\",\"può\",\"qua\",\"qualche\",\"qualcosa\",\"qualcuna\",\"qualcuno\",\"quale\",\"quali\",\"qualunque\",\"quando\",\"quanta\",\"quante\",\"quanti\",\"quanto\",\"quantunque\",\"quarto\",\"quasi\",\"quattro\",\"quel\",\"quella\",\"quelle\",\"quelli\",\"quello\",\"quest\",\"questa\",\"queste\",\"questi\",\"questo\",\"qui\",\"quindi\",\"quinto\",\"realmente\",\"recente\",\"recentemente\",\"registrazione\",\"relativo\",\"riecco\",\"rispetto\",\"salvo\",\"sara\",\"sarai\",\"saranno\",\"sarebbe\",\"sarebbero\",\"sarei\",\"saremmo\",\"saremo\",\"sareste\",\"saresti\",\"sarete\",\"sarà\",\"sarã\",\"sarò\",\"scola\",\"scopo\",\"scorso\",\"se\",\"secondo\",\"seguente\",\"seguito\",\"sei\",\"sembra\",\"sembrare\",\"sembrato\",\"sembrava\",\"sembri\",\"si\",\"sia\",\"siamo\",\"siano\",\"siate\",\"siete\",\"sig\",\"sono\",\"srl\",\"sta\",\"stai\",\"stando\",\"stanno\",\"starai\",\"staranno\",\"stiano\",\"stiate\",\"sto\",\"su\",\"sua\",\"subito\",\"successivamente\",\"successivo\",\"sue\",\"sugl\",\"sugli\",\"sui\",\"sul\",\"sull\",\"sulla\",\"sulle\",\"sullo\",\"suo\",\"suoi\",\"tale\",\"tali\",\"talvolta\",\"tanto\",\"te\",\"tempo\",\"th\",\"ti\",\"titolo\",\"tra\",\"tu\",\"tua\",\"tue\",\"tuo\",\"tuoi\",\"tutta\",\"tuttavia\",\"un\",\"una\",\"uno\",\"uomo\",\"va\",\"vai\",\"vale\",\"vari\",\"varia\",\"varie\",\"vario\",\"verso\",\"vi\",\"visto\",\"voi\",\"volta\",\"volte\",\"vostra\",\"vostre\",\"vostri\",\"vostro\",\"ã¨\",\"è\"]\n",
    "d1 = list(string.punctuation) + nltk.corpus.stopwords.words('italian')\n",
    "d2 = ['120', 'hotel', 'andare', 'recensioni', 'trattati','appeso','ottenere','sinistra', 'terminal', 'prezzo','aree','stile','staff','bagagli','iniziare','acca','fidanzato',':','stare','os','pochi','pur','scorsa','capitato','verità','trovati','bimba','qualsiasi','penso','lunga', '``','sentito','aggiunta','valeva','appena',\"c'era\",'lì','essa','?','pò','punti','“','”','...','’','entra','apparso','vie','ragazza','forma',\"'\",'sembravano','preso','compagno','é',\"l'abbiamo\",'..','provato','neanche',\"finita'\",\"''\",'davvero','s','nota','volete','deciso','parola','nonché',';','etc..','considerando', 'didnt','arriva']\n",
    "d3 = [\"a \", \"so\",'avec',\"adesso\",\"ai\",\"al\",\"alla\",\"allo\",\"allora\",\"altre\",\"altri\",\"altro\",\"anche\",\"ancora\",\"avere\",\"aveva\",\"avevano\",\"ben\",\"buono\",\"che\",\"chi\",\"cinque\",\"comprare\",\"con\",\"consecutivi\",\"consecutivo\",\"cosa\",\"cui\",\"da\",\"del\",\"della\",\"dello\",\"dentro\",\"deve\",\"devo\",\"di\",\"doppio\",\"due\",\"e\",\"ecco\",\"fare\",\"fine\",\"fino\",\"fra\",\"gente\",\"giu\",\"ha\",\"hai\",\"hanno\",\"ho \",\"il\",\"indietro\",\"invece\",\"io\",\"la\",\"lavoro\",\"le\",\"lei\",\"lo\",\"loro\",\"lui\",\"lungo\",\"ma\",\"me\",\"meglio\",\"molta\",\"molti\",\"molto\",\"nei\",\"nella\",\"no\",\"noi\",\"nome\",\"nostro\",\"nove\",\"nuovi\",\"nuovo\",\"o\",\"oltre\",\"ora\",\"otto\",\"pero\",\"persone\",\"piu\",\"poco\",\"primo\",\"promesso\",\"qua\",\"quarto\",\"quasi\",\"quattro\",\"quello\",\"questo\",\"qui\",\"quindi\",\"quinto\",\"sara\",\"secondo\",\"sei\",\"sembra\",\"sembrava\",\"senza\",\"sette\",\"sia\",\"siamo\",\"siete\",\"solo\",\"sono\",\"sopra \",\"soprattutto \",\"sotto\",\"stati\",\"stato \",\"stesso \",\"su \",\"subito\",\"sul\",\"sulla\",\"tanto\",\"te \",\"tempo\",\"terzo\",\"tra\",\"tre\",\"triplo\",\"ultimo\",\"un\",\"una\",\"uno\",\"va\",\"vai\",\"voi\",\"volte\",\"vostro\"]\n",
    "stop = list(set( d1 + d2+d3 + d5+d6))\n",
    "print(len(stop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop1 =  list(string.punctuation) + ['a', 'abbastanza', 'abbia', 'abbiamo', 'abbiano', 'abbiate', 'accidenti', 'ad', 'adesso', 'affinche', 'agl', 'agli', 'ahime', 'ahimã¨', 'ahimè', 'ai', 'al', 'alcuna', 'alcuni', 'alcuno', 'all', 'alla', 'alle', 'allo', 'allora', 'altre', 'altri', 'altrimenti', 'altro', 'altrove', 'altrui', 'anche', 'ancora', 'anni', 'anno', 'ansa', 'anticipo', 'assai', 'attesa', 'attraverso', 'avanti', 'avemmo', 'avendo', 'avente', 'aver', 'avere', 'averlo', 'avesse', 'avessero', 'avessi', 'avessimo', 'aveste', 'avesti', 'avete', 'aveva', 'avevamo', 'avevano', 'avevate', 'avevi', 'avevo', 'avrai', 'avranno', 'avrebbe', 'avrebbero', 'avrei', 'avremmo', 'avremo', 'avreste', 'avresti', 'avrete', 'avrà', 'avrò', 'avuta', 'avute', 'avuti', 'avuto', 'basta', 'ben', 'berlusconi', 'c', 'casa', 'caso', 'cento', 'certa', 'certe', 'certi', 'certo', 'che', 'chi', 'chicchessia', 'chiunque', 'ci', 'ciascuna', 'ciascuno', 'cima', 'cinque', 'cio', 'cioe', 'cioã¨', 'cioè', 'circa', 'ciã²', 'ciò', 'co', 'codesta', 'codesti', 'codesto', 'cogli', 'coi', 'col', 'colei', 'coll', 'coloro', 'colui', 'come', 'cominci', 'comprare', 'comunque', 'con', 'concernente', 'conciliarsi', 'conclusione', 'consecutivi', 'consecutivo', 'contro', 'cos', 'cosa', 'cosi', 'cosã¬', 'così', 'cui', 'd', 'da', 'dagl', 'dagli', 'dai', 'dal', 'dall', 'dalla', 'dalle', 'dallo', 'dappertutto', 'davanti', 'degl', 'degli', 'dei', 'del', 'dell', 'della', 'delle', 'dello', 'dentro', 'detto', 'deve', 'devo', 'di', 'dice', 'dietro', 'dire', 'dirimpetto', 'diventa', 'diventare', 'diventato', 'dopo', 'doppio', 'dov', 'dove', 'dovra', 'dovrà', 'dovrã', 'dovunque', 'due', 'dunque', 'durante', 'e', 'ebbe', 'ebbero', 'ebbi', 'ecc', 'ecco', 'ed', 'effettivamente', 'egli', 'ella', 'entrambi', 'eppure', 'era', 'erano', 'eravamo', 'eravate', 'eri', 'ero', 'esempio', 'esse', 'essendo', 'esser', 'essere', 'essi', 'ex', 'fa', 'faccia', 'facciamo', 'facciano', 'facciate', 'faccio', 'facemmo', 'facendo', 'facesse', 'facessero', 'facessi', 'facessimo', 'faceste', 'facesti', 'faceva', 'facevamo', 'facevano', 'facevate', 'facevi', 'facevo', 'fai', 'fanno', 'farai', 'faranno', 'fare', 'farebbe', 'farebbero', 'farei', 'faremmo', 'faremo', 'fareste', 'faresti', 'farete', 'farà', 'farò', 'fatto', 'favore', 'fece', 'fecero', 'feci', 'fin', 'finalmente', 'finche', 'fine', 'fino', 'forse', 'forza', 'fosse', 'fossero', 'fossi', 'fossimo', 'foste', 'fosti', 'fra', 'frattempo', 'fu', 'fui', 'fummo', 'fuori', 'furono', 'futuro', 'generale', 'gente', 'gia', 'giacche', 'giorni', 'giorno', 'giu', 'già', 'giã', 'gli', 'gliela', 'gliele', 'glieli', 'glielo', 'gliene', 'governo', 'grande', 'gruppo', 'ha', 'haha', 'hai', 'hanno', 'ho', 'i', 'ie', 'ieri', 'il', 'improvviso', 'in', 'inc', 'indietro', 'infatti', 'inoltre', 'insieme', 'intanto', 'intorno', 'invece', 'io', 'l', 'la', 'lasciato', 'lato', 'lavoro', 'le', 'lei', 'li', 'lo', 'lontano', 'loro', 'lui', 'lungo', 'luogo', 'là', 'lã', 'ma', 'macche', 'magari', 'maggior', 'mai', 'male', 'malgrado', 'malissimo', 'mancanza', 'marche', 'me', 'medesimo', 'mediante', 'meglio', 'meno', 'mentre', 'mesi', 'mezzo', 'mi', 'mia', 'mie', 'miei', 'mila', 'miliardi', 'milioni', 'minimi', 'ministro', 'mio', 'modo', 'momento', 'mondo', 'mosto', 'nazionale', 'ne', 'negl', 'negli', 'nei', 'nel', 'nell', 'nella', 'nelle', 'nello', 'no', 'noi', 'nome', 'non', 'nondimeno', 'nonostante', 'nonsia', 'nostra', 'nostre', 'nostri', 'nostro', 'novanta', 'nove', 'nulla', 'nuovi', 'nuovo', 'o', 'od', 'oggi', 'ogni', 'ognuna', 'ognuno', 'oltre', 'oppure', 'ora', 'ore', 'osi', 'ossia', 'ottanta', 'otto', 'paese', 'parecchi', 'parecchie', 'parecchio', 'parte', 'partendo', 'peccato', 'peggio', 'per', 'perche', 'perchã¨', 'perchè', 'perché', 'percio', 'perciã²', 'perciò', 'perfino', 'pero', 'persino', 'persone', 'perã²', 'però', 'piedi', 'pieno', 'piglia', 'piu', 'piuttosto', 'piã¹', 'più', 'po', 'poco', 'poi', 'poiche', 'possa', 'possedere', 'posteriore', 'posto', 'potrebbe', 'preferibilmente', 'presa', 'press', 'prima', 'primo', 'principalmente', 'probabilmente', 'promesso', 'proprio', 'puo', 'pure','purtroppo', 'puã²', 'può', 'qua', 'qualche', 'qualcosa', 'qualcuna', 'qualcuno', 'quale', 'quali', 'qualunque', 'quando', 'quanta', 'quante', 'quanti', 'quanto', 'quantunque', 'quarto', 'quasi', 'quattro', 'quel', 'quella', 'quelle', 'quelli', 'quello', 'quest', 'questa', 'queste', 'questi', 'questo', 'qui', 'quindi', 'quinto', 'realmente', 'recente', 'recentemente', 'registrazione', 'relativo', 'riecco', 'rispetto', 'salvo', 'sara', 'sarai', 'saranno', 'sarebbe', 'sarebbero', 'sarei', 'saremmo', 'saremo', 'sareste', 'saresti', 'sarete', 'sarà', 'sarã', 'sarò', 'scola', 'scopo', 'scorso', 'se', 'secondo', 'seguente', 'seguito', 'sei', 'sembra', 'sembrare', 'sembrato', 'sembrava', 'sembri', 'sempre', 'senza', 'sette', 'si', 'sia', 'siamo', 'siano', 'siate', 'siete', 'sig', 'solito', 'solo', 'soltanto', 'sono', 'sopra', 'soprattutto', 'sotto', 'spesso', 'srl', 'sta', 'stai', 'stando', 'stanno', 'starai', 'staranno', 'starebbe', 'starebbero', 'starei', 'staremmo', 'staremo', 'stareste', 'staresti', 'starete', 'starà', 'starò', 'stata', 'state', 'stati', 'stato', 'stava', 'stavamo', 'stavano', 'stavate', 'stavi', 'stavo', 'stemmo', 'stessa', 'stesse', 'stessero', 'stessi', 'stessimo', 'stesso', 'steste', 'stesti', 'stette', 'stettero', 'stetti', 'stia', 'stiamo', 'stiano', 'stiate', 'sto', 'su', 'sua', 'subito', 'successivamente', 'successivo', 'sue', 'sugl', 'sugli', 'sui', 'sul', 'sull', 'sulla', 'sulle', 'sullo', 'suo', 'suoi', 'tale', 'tali', 'talvolta', 'tanto', 'te', 'tempo', 'terzo', 'th', 'ti', 'titolo', 'torino', 'tra', 'tranne', 'tre', 'trenta', 'triplo', 'troppo', 'trovato', 'tu', 'tua', 'tue', 'tuo', 'tuoi', 'tutta', 'tuttavia', 'tutte', 'tutti', 'tutto', 'uguali', 'ulteriore', 'ultimo', 'un', 'una', 'uno', 'uomo', 'va', 'vai', 'vale', 'vari', 'varia', 'varie', 'vario', 'verso', 'vi', 'via', 'vicino', 'visto', 'vita', 'voi', 'volta', 'volte', 'vostra', 'vostre', 'vostri', 'vostro', 'ã¨', 'è', 'ahimã', 'cioã', 'cosã', 'perchã', 'assolutamente', 'veramente', 'dovuto', 'punto', 'presso', 'andare', 'chiedere', 'situata', 'trovare', 'prendere', 'appena', 'prenotato', 'arrivati', 'sicuramente', 'altra', 'potuto', 'dì', 'lì', 'pò', 'davvero', 'credo', 'almeno', 'chiesto', 'so', 'scelto', 'dovrebbe', 'bisogno', 'trova', 'dato', 'ok', 'rn', 'posso', 'viene', 'accanto', 'preso', 'andati']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmaTokenizer = LemmaTokenizer() \n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmaTokenizer, max_df = 0.5, min_df = 3, analyzer ='word',ngram_range = (1,4))\n",
    "#vectorizer =  CountVectorizer(tokenizer=lemmaTokenizer, max_df = 0.5, min_df = 3, analyzer ='word',ngram_range = (1,4))\n",
    "vectorizer.fit(dev['text'].values.tolist())\n",
    "vectors = vectorizer.transform(dev['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tf = TfidfTransformer()\n",
    "vectors1 = tf.fit_transform(vectors)\n",
    "vectors=vectors1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(dev['class'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev1 = list()\n",
    "dev2 = list()\n",
    "for doc in dev['text'].values:\n",
    "    dev2.append(np.array(re.sub('['+string.punctuation+']', ' ', doc.lower()).split()))\n",
    "    dev1.append(re.sub('['+string.punctuation+']', ' ', doc.lower()))\n",
    "dev1 = np.array(dev1)\n",
    "dev2 = np.array(dev2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 38\n",
      "qui\n"
     ]
    }
   ],
   "source": [
    "bad_words = ['capelli', 'assurdo', 'ingannevole', 'inesistenti', 'scarseggianti', 'sgarbato', 'indisponente', 'presuntuosamente', 'rivoltare', 'preservativo', 'anti-igienica',  'frettoloso', 'scadenti','losco', 'saccenti', '1/5','1/10' ,'sconsiglierei', 'inadeguato', 'imbevibile', 'orribile', 'labirintico', 'freddissime','peggior','abbandonata','luriodo', 'affolato', 'sgradevole','vergogna', 'sporco', 'squallido',  'caserma', 'lurida', 'maleducato', 'insopportabile', 'scontroso', 'sigaretta','peggior','sangue','schifo','sporca','degradante','briciole','squallida','odiare']\n",
    "good_words = ['rara', 'calorosa', 'insonorizzazione', 'superlativa', 'sorridente', 'vivamente', 'centralissimo'  'vicinissimo', 'economicissime',  'puntualissimi', 'confortevoli', 'competente',  'elegante', 'deliziosa','gentilissimo','affezionati', 'superlativo','sapientemente','eleganza','compiacere', 'straodinaria', 'lode', 'comode','benissimo','incantevole','lussuoso', 'comodissimo','brillanti','brillante','eccelente','raffinatissimo','incredbili','squisiti','perfetta','meraviglie', 'meraviglia','impeccabile','consigliatissimo','splendido']\n",
    "\n",
    "#bad_words = ['piccolo', 'squallido', 'caro', 'sporco' 'piccola', 'vergogna', 'digiuno', 'scadente ', 'terribile', 'abbandonata','caserma', 'gridando', 'vecchiume', 'sbattendo', 'lurida', 'macchiate','strappate' 'sottodimensionato','sconsigliato','maleducato ', 'poco pulite', 'inferiore', 'non merita', 'problemi','migliorare','difficile', ' mediocre','polvere','plastica','gradini','negative','affollato','stretta', 'limitata','insopportabile','sporca','briciole','deludente','freddo','sgradevole','piccolissima','fastidiosa','difficoltà','finta','carissimo','tassa','maleodoranti', 'povero','brutto','capelli', 'labirinto','impersonale', 'non torneremo','non inclusa','poco pulita','non funzionava','non pulito','non','modesto','scontroso','difetto', 'non tornerò', 'puzzava','fumo','sigaretta','assurdo','poco confortevole', 'vecchio','rotti','rotta','cattivo','peggior','sangue','ospedale','puzza','schifo','scorbutico','decenza']\n",
    "#good_words = ['lussuoso', 'cordiale', 'perfetta', 'pulitissime', 'incantevole', 'stupendo', 'spaziosa', 'fantastico', 'comodo', 'tranquillissima' ,'rinnovato' ,'efficiente', 'eccezionale', 'ottimo', 'speciale', 'sapientemente', 'grande','ben','buono','eccellente','bel','affezionati', 'superiore', 'belle','buone','splendida','elegantemente','buona','ideale','gentile','disponibile','rara','pulite','confortevoli', 'elegante','ottima','ritornerò', 'ottimo','cortese','superlativo','bella','ampia','pulita','comodissima','accogliente','comoda','carino','gentilissimo','neo','ritorneremo', 'splendide','buon','pulito','complimenti','eleganza','eccellenti','incredibile','stupenda','meraviglioso','suite','efficienti','grazie','incredibili','disponibili','bellissima','nessun difetto', 'deliziosi','squisiti','tornerò','bello','confortevole','perfetta','meraviglie','impeccabile','fantastica']\n",
    "\n",
    "bad_words = np.array(list(set(bad_words)))\n",
    "good_words = np.array(list(set(good_words)))\n",
    "\n",
    "print(len(bad_words), len(good_words))\n",
    "features = np.zeros((len(dev),2))\n",
    "\n",
    "for good in good_words:\n",
    "    for i, doc in enumerate(dev2):\n",
    "        if good in doc:\n",
    "            features[i][0]+=1\n",
    "\n",
    "print('qui')\n",
    "for bad in bad_words:\n",
    "    for i, doc in enumerate(dev2):\n",
    "        if bad in  doc:\n",
    "            features[i][1]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "\n",
    "for i, val in enumerate(dev['text']):\n",
    "    doc = nlp(val)\n",
    "    print(i)\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ':\n",
    "            features[i][2]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectors\n",
    "X = csr_matrix(X)\n",
    "X  = hstack([X, features])\n",
    "#X = hstack([X, appo.reshape(X.shape[0], 1)])\n",
    "X = csr_matrix(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "X, y = BorderlineSMOTE().fit_resample(X, y)\n",
    "\n",
    "#smote_nc = SMOTENC(categorical_features = [0,1], random_state=0)\n",
    "#X, y = smote_nc.fit_resample(X, y)\n",
    "#X, y = ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS 0.918601867368925\n",
      "NEG 0.9379912663755459\n",
      "10644\n"
     ]
    }
   ],
   "source": [
    "pos = 0\n",
    "neg = 0\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "for i, val in enumerate(features):\n",
    "    if val[0] > val[1]:\n",
    "        count1+=1\n",
    "        if y[i] == 1:\n",
    "            pos+=1\n",
    "    if val[1] > val[0]:\n",
    "        count2+=1\n",
    "        if y[i] == 0:\n",
    "            neg+=1\n",
    "print(f\"POS {pos/count1}\")\n",
    "print(f\"NEG {neg/count2}\")\n",
    "print(count1+count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import KBinsDiscretizer#\n",
    "#chi2sampler = AdditiveChi2Sampler(sample_steps=2)\n",
    "#X = chi2sampler.fit_transform(X, y)\n",
    "s = SelectKBest(chi2, k=150000)\n",
    "#est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "#X = est.fit_transform(X)\n",
    "#X = preprocessing.StandardScaler(with_mean = False).fit_transform(X)\n",
    "#s = SelectFpr(chi2, alpha=0.2)\n",
    "#s = SelectPercentile(chi2, percentile=40)\n",
    "#embeded_rf_selector = SelectFromModel(LinearSVC(dual=False, class_weight='balanced'))\n",
    "#embeded_rf_selector.fit(X, y)\n",
    "#embeded_rf_selector.get_support()\n",
    "\n",
    "X = s.fit_transform(X, y)\n",
    "\n",
    "#scaler = MaxAbsScaler()\n",
    "#X = scaler.fit_transform(X)\n",
    "#model =  LinearSVC()\n",
    "#rfe = RFE(model, 100)\n",
    "#X = rfe.fit_transform(X, y)\n",
    "#X = winsorize(X, limits = 0.01)\n",
    "#lsvc = LinearSVC().fit(X, y)\n",
    "#clf = LassoCV()\n",
    "#model = SelectFromModel(lsvc, prefit=False)\n",
    "#X = model.fit_transform(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "svd = TruncatedSVD(n_components=150, algorithm = 'arpack', tol = 1e-15)\n",
    "X_projection = svd.fit_transform(vectors)\n",
    "plt.plot(svd.explained_variance_ratio_, marker='o', linestyle='') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "svd = TruncatedSVD(n_components=80, algorithm = 'arpack', tol = 1e-15)\n",
    "#pca = SparsePCA(n_components=15)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "X = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28754, 481887)"
      ]
     },
     "execution_count": 1200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9473057457576659\n",
      "[[1885  474]\n",
      " [  57 4773]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "clf = RandomForestClassifier(n_estimators = 100, random_state=2, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred2 = clf.predict(X_test)\n",
    "print(f1_score(y_test, y_pred2))\n",
    "print(confusion_matrix(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8057734023026865\n",
      "[[  32 2327]\n",
      " [   1 4829]]\n"
     ]
    }
   ],
   "source": [
    "kne = KNeighborsClassifier(n_jobs = -1, n_neighbors=7)\n",
    "kne.fit(X_train, y_train)\n",
    "y_pred = kne.predict(X_test)\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = PolynomialFeatures(2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "lsvc = LinearSVC(C = 3.5, random_state = 3, dual = False,  max_iter=20000, intercept_scaling = 0.0001, class_weight='balanced')\n",
    "#svc = LinearSVC(C =33 random_state = 3,   dual=False,  intercept_scaling = 0.001, class_weight='balanced')\n",
    "\n",
    "lsvc.fit(X_train, y_train)\n",
    "y_pred = lsvc.predict(X_test)\n",
    "\n",
    "#scores = cross_val_score(lsvc, X, y, cv=5, scoring='f1_weighted', n_jobs = -1)\n",
    "#print(scores)\n",
    "#print(scores.mean())\n",
    "print(f1_score(y_test, y_pred, average = \"weighted\"))\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9741039983426559\n",
      "[[2237  122]\n",
      " [ 128 4702]]\n"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression(C =3, warm_start = True,random_state = 3,  multi_class = 'ovr',n_jobs = 8, max_iter = 1000, tol = 1e-9,  class_weight='balanced', dual=False, intercept_scaling = 0.001)\n",
    "lg.fit(X_train, y_train)\n",
    "y_pred = lg.predict(X_test)\n",
    "\n",
    "#scores = cross_val_score(lg, X, y, cv=5, scoring='f1_macro', n_jobs = -1)\n",
    "#print(scores)\n",
    "#print(scores.mean())\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.980830343413634\n",
      "[[2218  141]\n",
      " [  46 4784]]\n"
     ]
    }
   ],
   "source": [
    "clf = PassiveAggressiveClassifier(max_iter=100, random_state=1, tol=1e-3, C = 2, n_jobs = -1, fit_intercept = True)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 59.21, NNZs: 138004, Bias: 0.170988, T: 21565, Avg. loss: 0.265743\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 55.33, NNZs: 146519, Bias: 0.168211, T: 43130, Avg. loss: 0.153917\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 54.69, NNZs: 149245, Bias: 0.177868, T: 64695, Avg. loss: 0.144174\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 54.31, NNZs: 150311, Bias: 0.216224, T: 86260, Avg. loss: 0.138242\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 54.16, NNZs: 150971, Bias: 0.193145, T: 107825, Avg. loss: 0.134548\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 54.06, NNZs: 151443, Bias: 0.191371, T: 129390, Avg. loss: 0.132885\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 54.00, NNZs: 151689, Bias: 0.191979, T: 150955, Avg. loss: 0.130809\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 53.96, NNZs: 151879, Bias: 0.188514, T: 172520, Avg. loss: 0.129940\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 53.90, NNZs: 152029, Bias: 0.186992, T: 194085, Avg. loss: 0.128862\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 53.93, NNZs: 152135, Bias: 0.187805, T: 215650, Avg. loss: 0.128268\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 53.90, NNZs: 152218, Bias: 0.191568, T: 237215, Avg. loss: 0.127957\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 53.88, NNZs: 152306, Bias: 0.190283, T: 258780, Avg. loss: 0.127184\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 53.89, NNZs: 152362, Bias: 0.201341, T: 280345, Avg. loss: 0.127038\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 53.90, NNZs: 152385, Bias: 0.198339, T: 301910, Avg. loss: 0.126523\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 53.87, NNZs: 152425, Bias: 0.194143, T: 323475, Avg. loss: 0.126122\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 53.88, NNZs: 152490, Bias: 0.187813, T: 345040, Avg. loss: 0.125849\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 53.88, NNZs: 152514, Bias: 0.196824, T: 366605, Avg. loss: 0.125565\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 53.85, NNZs: 152539, Bias: 0.199837, T: 388170, Avg. loss: 0.125311\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 53.87, NNZs: 152555, Bias: 0.194645, T: 409735, Avg. loss: 0.125310\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 53.86, NNZs: 152563, Bias: 0.195930, T: 431300, Avg. loss: 0.125026\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 53.85, NNZs: 152585, Bias: 0.195655, T: 452865, Avg. loss: 0.124738\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 53.85, NNZs: 152592, Bias: 0.195248, T: 474430, Avg. loss: 0.124727\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 53.86, NNZs: 152596, Bias: 0.198241, T: 495995, Avg. loss: 0.124660\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 53.87, NNZs: 152605, Bias: 0.194655, T: 517560, Avg. loss: 0.124320\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 53.86, NNZs: 152605, Bias: 0.193213, T: 539125, Avg. loss: 0.124131\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 53.86, NNZs: 152605, Bias: 0.193688, T: 560690, Avg. loss: 0.124071\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 53.86, NNZs: 152613, Bias: 0.196283, T: 582255, Avg. loss: 0.124142\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 53.86, NNZs: 152647, Bias: 0.194253, T: 603820, Avg. loss: 0.123948\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 53.87, NNZs: 152652, Bias: 0.194055, T: 625385, Avg. loss: 0.123930\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 53.86, NNZs: 152673, Bias: 0.195349, T: 646950, Avg. loss: 0.123740\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 53.85, NNZs: 152688, Bias: 0.196456, T: 668515, Avg. loss: 0.123791\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 53.86, NNZs: 152706, Bias: 0.195822, T: 690080, Avg. loss: 0.123604\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 53.85, NNZs: 152709, Bias: 0.196077, T: 711645, Avg. loss: 0.123420\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 53.85, NNZs: 152718, Bias: 0.195647, T: 733210, Avg. loss: 0.123583\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 53.86, NNZs: 152725, Bias: 0.195387, T: 754775, Avg. loss: 0.123456\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 53.86, NNZs: 152729, Bias: 0.191622, T: 776340, Avg. loss: 0.123510\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 53.85, NNZs: 152729, Bias: 0.196604, T: 797905, Avg. loss: 0.123276\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 53.85, NNZs: 152729, Bias: 0.199300, T: 819470, Avg. loss: 0.123289\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 53.86, NNZs: 152736, Bias: 0.194428, T: 841035, Avg. loss: 0.123293\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 53.86, NNZs: 152736, Bias: 0.195205, T: 862600, Avg. loss: 0.123297\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 53.86, NNZs: 152736, Bias: 0.196104, T: 884165, Avg. loss: 0.123179\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 53.86, NNZs: 152736, Bias: 0.194214, T: 905730, Avg. loss: 0.122871\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 53.86, NNZs: 152736, Bias: 0.195456, T: 927295, Avg. loss: 0.123111\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 53.86, NNZs: 152736, Bias: 0.195564, T: 948860, Avg. loss: 0.123106\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 53.86, NNZs: 152736, Bias: 0.196031, T: 970425, Avg. loss: 0.122926\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 53.85, NNZs: 152736, Bias: 0.196623, T: 991990, Avg. loss: 0.122902\n",
      "Total training time: 0.69 seconds.\n",
      "Convergence after 46 epochs took 0.69 seconds\n",
      "0.9773221497359428\n",
      "[[2251  108]\n",
      " [ 111 4719]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "clf = linear_model.SGDClassifier(max_iter=1000,  warm_start = True, tol=1e-15, n_iter_no_change = 4,   n_jobs=-1,  random_state = 1, verbose = True, class_weight = 'balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators = 1000,  max_depth = 15, max_features = 'log2', min_samples_leaf = 0.001, random_state = 1)\n",
    "y_pred1 = gbc.fit(X_train, y_train).predict(X_test)\n",
    "print(f1_score<(y_test, y_pred1))\n",
    "print(confusion_matrix(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Given matrix X and min_npoints, plot K-Neighbors chart\n",
    "# It also plots a horizontal line in correspondence to eps value\n",
    "def plot_neighbors(X, min_points, eps=0):\n",
    "    nbrs = NearestNeighbors(n_neighbors=min_points + 1).fit(X)\n",
    "    distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "    k_dist = distances[:, -1]\n",
    "    k_dist.sort()\n",
    "    x_axis = np.arange(k_dist.shape[0])\n",
    "\n",
    "    ### Disable comment below to activate interactive plots\n",
    "    #%matplotlib notebook\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, k_dist, linestyle='', marker='o', markersize=1)\n",
    "    \n",
    "    # Plot a horizontal line in correspondence to eps value\n",
    "    ax.hlines(eps, x_axis.min(), x_axis.max(), linestyle='--')\n",
    "    ax.set_title(f\"K-Neighbors chart. Min_points = {min_points}, eps={eps}.\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_neighbors(X, 50, eps=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_db10 = DBSCAN(eps = 1.3, min_samples=50)\n",
    "res_db10 = model_db10.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
      "       16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\n",
      "       33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "       50, 51, 52, 53, 54, 55, 56, 57], dtype=int64), array([ 1645,  2683,   866, 12854,   352,   237,   363,    84,   300,\n",
      "         216,   309,   564,   142,    75,   234,   168,    91,   568,\n",
      "         273,   208,    82,   198,   398,   781,   661,   143,    71,\n",
      "         173,   294,   431,    98,    79,   148,   121,   164,    61,\n",
      "          56,   384,   108,   240,    69,   176,   119,   154,   526,\n",
      "         102,   162,    69,   388,   142,    55,    82,    92,   105,\n",
      "         102,    54,    59,    60,    53], dtype=int64))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(29492, 194671)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.unique(res_db10,return_counts = True))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  = hstack([X, res_db10.reshape(X.shape[0], 1)])\n",
    "X = csr_matrix(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = X[ res_db10 != -1 ]\n",
    "y = y[ res_db10 != -1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1250,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_ev = vectorizer.transform(ev['text'].values.tolist())\n",
    "ev1 = list()\n",
    "ev2 = list()\n",
    "for doc in ev['text'].values:\n",
    "    ev2.append(np.array(re.sub('['+string.punctuation+']', ' ', doc.lower()).split()))\n",
    "    ev1.append(re.sub('['+string.punctuation+']', ' ', doc.lower()))\n",
    "ev1 = np.array(ev1)\n",
    "#vectors_ev = lsa.transform(vectors_ev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_ev = tf.transform(vectors_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qui\n"
     ]
    }
   ],
   "source": [
    "features_ev = np.zeros((len(ev),2))\n",
    "for good in good_words:\n",
    "    for i, doc in enumerate(ev2):\n",
    "        if good in doc:\n",
    "            features_ev[i][0]+=1\n",
    "print('qui')        \n",
    "\n",
    "for bad in bad_words:\n",
    "    for i, doc in enumerate(ev2):\n",
    "        if bad in doc:\n",
    "            features_ev[i][1]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1253,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ev = csr_matrix(X_ev)\n",
    "X_ev  = hstack([X_ev,features_ev])\n",
    "X_ev = csr_matrix(X_ev)\n",
    "X_ev = s.transform(X_ev)\n",
    "\n",
    "#X_ev = model.transform(X_ev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features_ev = np.zeros((len(ev),2))\n",
    "\n",
    "for good in goods:\n",
    "    for i, doc in enumerate(ev2):\n",
    "        if good in doc:\n",
    "            features_ev[i][0]+=1\n",
    "\n",
    "print('qui')\n",
    "for bad in bads:\n",
    "    for i, doc in enumerate(ev2):\n",
    "        if bad in  doc:\n",
    "            features_ev[i][1]+=1\n",
    "            \n",
    "\n",
    "X_ev  = hstack([X_ev,features_ev])\n",
    "X_ev = csr_matrix(X_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1254,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(C = 3.5, random_state = 3, dual=False, max_iter=20000, intercept_scaling = 0.0001, class_weight='balanced')\n",
    "#lg = LogisticRegression(C =2, warm_start = True,random_state = 3,  multi_class = 'ovr',n_jobs = 8, max_iter = 1000, tol = 1e-9,  class_weight='balanced', dual=False, intercept_scaling = 0.001)\n",
    "\n",
    "#svc = svm.SVC(C=0.6, kernel='linear',  gamma='scale', random_state=2)\n",
    "#svc = RandomForestClassifier(n_estimators = 1000, random_state=2, n_jobs=-1)\n",
    "#svc = GradientBoostingClassifier(n_estimators = 1000, max_depth = 12, max_features = 'log2', min_samples_leaf = 0.001, random_state = 1)\n",
    "#clf = linear_model.SGDClassifier(max_iter=10000,  warm_start = True, tol=1e-12, n_iter_no_change = 4,  alpha = 0.00001, n_jobs=-1,  random_state = 1, verbose = True, class_weight = 'balanced')\n",
    "\n",
    "lsvc.fit(X, y)\n",
    "res_v = lsvc.predict(X_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1255,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.csv', 'w') as file:\n",
    "    file.write('Id,Predicted\\n')\n",
    "    for i in range(len(res_v)):\n",
    "        if res_v[i] == 1:\n",
    "            file.write(f\"{i},pos\\n\")\n",
    "        else:\n",
    "            file.write(f\"{i},neg\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([3858, 8465], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(res_v,return_counts = True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
